{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xTXy4DVi4ok"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch_lightning\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGCAhcUR2MLs",
        "outputId": "a065bb5f-2ed0-44c4-b36c-95d0be6bfe6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Mar 13 18:15:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b4cYkUwQjEOj",
        "outputId": "2fb2df3b-0bc2-4f9c-cb1e-75a9e7dc934a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.10.0+cu111'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import Dataset , DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from itertools import chain\n",
        "import ast\n",
        "from transformers import T5TokenizerFast\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "torch.version.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foO3yY9wjB5H",
        "outputId": "73c87b3a-0390-46f8-ddf8-fdb8b9f5171e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "root = \"drive/MyDrive/lign167_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "284qZEqIw0fq"
      },
      "source": [
        "# 1. Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5q0pcNOjy4N"
      },
      "outputs": [],
      "source": [
        "raw_data = pd.read_csv(f\"{root}/sampled_999986.csv\", converters={1:ast.literal_eval})\n",
        "raw_data = raw_data[[\"sents\"]]\n",
        "raw_data = raw_data.sample(n=500000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQkH0VMRkQsQ",
        "outputId": "fac834b3-afa5-4b4c-cddc-3fbc131ab62c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1355200\n"
          ]
        }
      ],
      "source": [
        "print(raw_data[\"sents\"].apply(lambda x: len(x) - 1).sum())\n",
        "raw_data[\"cumlen\"] = raw_data[\"sents\"].apply(lambda x: len(x) - 1).cumsum() - 1\n",
        "raw_data[\"len\"] = raw_data[\"sents\"].apply(lambda x: len(x) - 1)\n",
        "raw_data = raw_data.set_index(\"cumlen\")\n",
        "\n",
        "pd.options.display.max_colwidth = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f0v4iFqjiTC"
      },
      "outputs": [],
      "source": [
        "# initialize tokenizer for Dataset building\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0GQ7xwCjFwz"
      },
      "outputs": [],
      "source": [
        "sent_length = 32\n",
        "class AmazonDataset(Dataset):\n",
        "    def __init__(self,data):\n",
        "        self.data = data\n",
        "        self.len = raw_data[\"sents\"].apply(lambda x: len(x) - 1).sum()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def to_token(self,sentence):\n",
        "        return tokenizer.encode(sentence, max_length=sent_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")[0]\n",
        "    \n",
        "    def get_pair(self, idx):\n",
        "      iidx = idx\n",
        "      while iidx not in raw_data.index:\n",
        "        iidx += 1\n",
        "      line = raw_data[\"sents\"].loc[iidx]\n",
        "      base = idx - iidx - 2\n",
        "      return (line[base], line[base + 1])\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        context, input = self.get_pair(index)\n",
        "        return self.to_token(context), self.to_token(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YoGVae3vYfj",
        "outputId": "08116aec-e19b-492d-c5f7-8ed976194968"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ 100,  556,   19, 2033,   91,   13,  833,   11,    8, 4818,  163, 4951,\n",
              "           95,   12, 1758,    3,    4,  345,    5,    1,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " tensor([  94,  930, 8957,   16, 8217,    6,   68,  405,   59,  161,   16, 6687,\n",
              "           18, 2360, 1758,  489,   41, 7965,   59, 5285, 3538,   18, 2360, 1758,\n",
              "         7973,   68,    8, 4818,  405,   59,  570,    1]))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AmazonDataset(raw_data)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OscQSONTvgiN"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "class AmazonDataModule(pl.LightningDataModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        train_dataset, val_dataset = train_test_split(raw_data, test_size=0.01)\n",
        "        self.train = AmazonDataset(train_dataset)\n",
        "        self.test = AmazonDataset(val_dataset)\n",
        "        self.val = AmazonDataset(val_dataset)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train , batch_size = batch_size , shuffle = True, num_workers=4)\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test , batch_size = batch_size , shuffle = False, num_workers=4)\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val , batch_size = batch_size , shuffle = False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clKMWEiGxbbs"
      },
      "source": [
        "# Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHGrrth8whTK"
      },
      "outputs": [],
      "source": [
        "from transformers.models.t5.modeling_t5 import T5Stack, T5PreTrainedModel\n",
        "from transformers.modeling_outputs import (BaseModelOutput,\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput,\n",
        "    Seq2SeqQuestionAnsweringModelOutput,\n",
        "    Seq2SeqSequenceClassifierOutput,)\n",
        "from transformers.models.t5.configuration_t5 import T5Config\n",
        "from transformers.utils.model_parallel_utils import get_device_map, assert_device_map\n",
        "import warnings\n",
        "import copy\n",
        "\n",
        "__HEAD_MASK_WARNING_MSG = \"\"\"\n",
        "The input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n",
        "`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\n",
        "If you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\n",
        "num_heads)`.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuLN-pOSx8er"
      },
      "outputs": [],
      "source": [
        "lambda_factor = 1\n",
        "class T5ForConditionalGenerationWithExtractor(T5PreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [\n",
        "        r\"encoder\\.embed_tokens\\.weight\",\n",
        "        r\"decoder\\.embed_tokens\\.weight\",\n",
        "        r\"lm_head\\.weight\",\n",
        "    ]\n",
        "    _keys_to_ignore_on_load_unexpected = [\n",
        "        r\"decoder\\.block\\.0\\.layer\\.1\\.EncDecAttention\\.relative_attention_bias\\.weight\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model_dim = config.d_model\n",
        "\n",
        "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
        "\n",
        "        encoder_config = copy.deepcopy(config)\n",
        "        encoder_config.is_decoder = False\n",
        "        encoder_config.use_cache = False\n",
        "        encoder_config.is_encoder_decoder = False\n",
        "        self.encoder = T5Stack(encoder_config, self.shared)\n",
        "\n",
        "        extractor_config = copy.deepcopy(config)\n",
        "        extractor_config.is_decoder = False\n",
        "        extractor_config.use_cache = False\n",
        "        extractor_config.is_encoder_decoder = False\n",
        "        self.extractor = T5Stack(extractor_config, self.shared)\n",
        "\n",
        "        decoder_config = copy.deepcopy(config)\n",
        "        decoder_config.is_decoder = True\n",
        "        decoder_config.is_encoder_decoder = False\n",
        "        decoder_config.num_layers = config.num_decoder_layers\n",
        "        self.decoder = T5Stack(decoder_config, self.shared)\n",
        "\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "        # Model parallel\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "\n",
        "    def parallelize(self, device_map=None):\n",
        "        self.device_map = (\n",
        "            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n",
        "            if device_map is None\n",
        "            else device_map\n",
        "        )\n",
        "        assert_device_map(self.device_map, len(self.encoder.block))\n",
        "        self.encoder.parallelize(self.device_map)\n",
        "        self.decoder.parallelize(self.device_map)\n",
        "        self.extractor.parallelize(self.device_map)\n",
        "        self.lm_head = self.lm_head.to(self.decoder.first_device)\n",
        "        self.model_parallel = True\n",
        "\n",
        "    def deparallelize(self):\n",
        "        self.encoder.deparallelize()\n",
        "        self.extractor.deparallelize()\n",
        "        self.decoder.deparallelize()\n",
        "        self.encoder = self.encoder.to(\"cpu\")\n",
        "        self.extractor = self.extractor.to(\"cpu\")\n",
        "        self.decoder = self.decoder.to(\"cpu\")\n",
        "        self.lm_head = self.lm_head.to(\"cpu\")\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.shared = new_embeddings\n",
        "        self.encoder.set_input_embeddings(new_embeddings)\n",
        "        self.extractor.set_input_embeddings(new_embeddings)\n",
        "        self.decoder.set_input_embeddings(new_embeddings)\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "    \n",
        "    def get_extractor(self):\n",
        "        return self.extractor\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def get_extractor_output(self,\n",
        "        input_ids=None,\n",
        "        use_cache_context_ids=None, # use cache is simply to a trick to use the generator mixin\n",
        "        use_cache_target_examplars_ids=None,\n",
        "        use_cache_origin_examplars_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        extractor_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        context_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,):\n",
        "      extractor_hidden = None\n",
        "      if use_cache_context_ids is None:\n",
        "        target_styles = ()\n",
        "        for target_ids in use_cache_target_examplars_ids:\n",
        "          extractor_hidden = self.extractor(\n",
        "              input_ids=target_ids,\n",
        "              attention_mask=attention_mask,\n",
        "              inputs_embeds=context_embeds,\n",
        "              head_mask=head_mask,\n",
        "              output_attentions=output_attentions,\n",
        "              output_hidden_states=output_hidden_states,\n",
        "              return_dict=return_dict,\n",
        "          )[0]\n",
        "          target_styles += (extractor_hidden,)\n",
        "\n",
        "        original_styles = ()\n",
        "        for origin_ids in use_cache_origin_examplars_ids:\n",
        "          extractor_hidden = self.extractor(\n",
        "              input_ids=origin_ids,\n",
        "              attention_mask=attention_mask,\n",
        "              inputs_embeds=context_embeds,\n",
        "              head_mask=head_mask,\n",
        "              output_attentions=output_attentions,\n",
        "              output_hidden_states=output_hidden_states,\n",
        "              return_dict=return_dict,\n",
        "          )[0]\n",
        "          original_styles += (extractor_hidden,)\n",
        "          \n",
        "        input_style = self.extractor(\n",
        "              input_ids=input_ids,\n",
        "              attention_mask=attention_mask,\n",
        "              inputs_embeds=context_embeds,\n",
        "              head_mask=head_mask,\n",
        "              output_attentions=output_attentions,\n",
        "              output_hidden_states=output_hidden_states,\n",
        "              return_dict=return_dict,\n",
        "          )[0]\n",
        "        extractor_hidden = lambda_factor * (torch.mean(torch.vstack(target_styles), 0) - (torch.mean(torch.vstack(original_styles), 0))) + input_style\n",
        "      \n",
        "      else:\n",
        "        if extractor_outputs is None:\n",
        "            extractor_outputs = self.extractor(\n",
        "                input_ids=use_cache_context_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                inputs_embeds=context_embeds,\n",
        "                head_mask=head_mask,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            extractor_outputs = BaseModelOutput(\n",
        "                last_hidden_state=extractor_outputs[0],\n",
        "                hidden_states=extractor_outputs[1] if len(extractor_outputs) > 1 else None,\n",
        "                attentions=extractor_outputs[2] if len(extractor_outputs) > 2 else None,)\n",
        "        extractor_hidden = extractor_outputs[0]\n",
        "      return extractor_hidden\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        use_cache_extractor_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        context_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n",
        "            config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n",
        "            labels in `[0, ..., config.vocab_size]`\n",
        "        Returns:\n",
        "        Examples:\n",
        "        ```python\n",
        "        >>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "        >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "        >>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "        >>> # training\n",
        "        >>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
        "        >>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
        "        >>> outputs = model(input_ids=input_ids, labels=labels)\n",
        "        >>> loss = outputs.loss\n",
        "        >>> logits = outputs.logits\n",
        "        >>> # inference\n",
        "        >>> input_ids = tokenizer(\n",
        "        ...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
        "        >>> ).input_ids  # Batch size 1\n",
        "        >>> outputs = model.generate(input_ids)\n",
        "        >>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "        >>> # studies have shown that owning a dog is good for you.\n",
        "        ```\"\"\"\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
        "        if head_mask is not None and decoder_head_mask is None:\n",
        "            if self.config.num_layers == self.config.num_decoder_layers:\n",
        "                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n",
        "                decoder_head_mask = head_mask\n",
        "\n",
        "        # Encode if needed (training, first prediction pass)\n",
        "        if encoder_outputs is None:\n",
        "            # Convert encoder inputs in embeddings if needed\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                head_mask=head_mask,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        hidden_states = encoder_outputs[0] + use_cache_extractor_outputs\n",
        "\n",
        "        if self.model_parallel:\n",
        "            torch.cuda.set_device(self.decoder.first_device)\n",
        "\n",
        "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "            # get decoder inputs from shifting lm labels to the right\n",
        "            decoder_input_ids = self._shift_right(labels)\n",
        "\n",
        "        # Set device for model parallelism\n",
        "        if self.model_parallel:\n",
        "            torch.cuda.set_device(self.decoder.first_device)\n",
        "            hidden_states = hidden_states.to(self.decoder.first_device)\n",
        "            if decoder_input_ids is not None:\n",
        "                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
        "            if attention_mask is not None:\n",
        "                attention_mask = attention_mask.to(self.decoder.first_device)\n",
        "            if decoder_attention_mask is not None:\n",
        "                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
        "\n",
        "        # Decode\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            past_key_values=past_key_values,\n",
        "            encoder_hidden_states=hidden_states,\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = decoder_outputs[0]\n",
        "\n",
        "        # Set device for model parallelism\n",
        "        if self.model_parallel:\n",
        "            torch.cuda.set_device(self.encoder.first_device)\n",
        "            self.lm_head = self.lm_head.to(self.encoder.first_device)\n",
        "            sequence_output = sequence_output.to(self.lm_head.weight.device)\n",
        "\n",
        "        if self.config.tie_word_embeddings:\n",
        "            # Rescale output before projecting on vocab\n",
        "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
        "            sequence_output = sequence_output * (self.model_dim**-0.5)\n",
        "\n",
        "        lm_logits = self.lm_head(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
        "            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        input_ids,\n",
        "        use_cache_extractor_outputs=None,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        use_cache=None,\n",
        "        encoder_outputs=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        return {\n",
        "            # \"input_ids\": input_ids,\n",
        "            # \"use_cache_context_ids\": use_cache_context_ids,\n",
        "            # \"use_cache_target_examplars_ids\": use_cache_target_examplars_ids,\n",
        "            # \"use_cache_origin_examplars_ids\": use_cache_origin_examplars_ids,\n",
        "            \"decoder_input_ids\": input_ids,\n",
        "            \"past_key_values\": past,\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"use_cache_extractor_outputs\": use_cache_extractor_outputs,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"head_mask\": head_mask,\n",
        "            \"decoder_head_mask\": decoder_head_mask,\n",
        "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
        "            \"use_cache\": use_cache,\n",
        "        }\n",
        "\n",
        "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
        "        return self._shift_right(labels)\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        # if decoder past is not included in output\n",
        "        # speedy decoding is disabled and no need to reorder\n",
        "        if past is None:\n",
        "            warnings.warning(\"You might want to consider setting `use_cache=True` to speed up decoding\")\n",
        "            return past\n",
        "\n",
        "        reordered_decoder_past = ()\n",
        "        for layer_past_states in past:\n",
        "            # get the correct batch idx from layer past batch dim\n",
        "            # batch dim of `past` is at 2nd position\n",
        "            reordered_layer_past_states = ()\n",
        "            for layer_past_state in layer_past_states:\n",
        "                # need to set correct `past` for each of the four key / value states\n",
        "                reordered_layer_past_states = reordered_layer_past_states + (\n",
        "                    layer_past_state.index_select(0, beam_idx.to(layer_past_state.device)),\n",
        "                )\n",
        "\n",
        "            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n",
        "            assert len(reordered_layer_past_states) == len(layer_past_states)\n",
        "\n",
        "            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n",
        "        return reordered_decoder_past\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anL1dLwzyqsO",
        "outputId": "63b405b1-a1f5-41e6-e686-6142c526ab15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of T5ForConditionalGenerationWithExtractor were not initialized from the model checkpoint at t5-base and are newly initialized: ['extractor.block.4.layer.0.SelfAttention.o.weight', 'extractor.block.5.layer.0.layer_norm.weight', 'extractor.block.8.layer.0.SelfAttention.q.weight', 'extractor.block.1.layer.1.layer_norm.weight', 'extractor.block.1.layer.1.DenseReluDense.wo.weight', 'extractor.block.11.layer.0.layer_norm.weight', 'extractor.block.8.layer.1.layer_norm.weight', 'extractor.block.6.layer.0.SelfAttention.o.weight', 'extractor.block.0.layer.0.layer_norm.weight', 'extractor.block.0.layer.0.SelfAttention.o.weight', 'extractor.block.6.layer.1.DenseReluDense.wi.weight', 'extractor.block.3.layer.1.DenseReluDense.wo.weight', 'extractor.block.4.layer.0.layer_norm.weight', 'extractor.block.1.layer.0.SelfAttention.k.weight', 'extractor.block.3.layer.0.SelfAttention.o.weight', 'extractor.block.7.layer.0.SelfAttention.k.weight', 'extractor.block.7.layer.0.SelfAttention.q.weight', 'extractor.block.3.layer.1.layer_norm.weight', 'extractor.block.8.layer.0.SelfAttention.v.weight', 'extractor.block.2.layer.1.DenseReluDense.wi.weight', 'extractor.block.1.layer.0.SelfAttention.o.weight', 'extractor.block.4.layer.1.layer_norm.weight', 'extractor.block.2.layer.0.SelfAttention.q.weight', 'extractor.block.5.layer.1.layer_norm.weight', 'extractor.block.2.layer.0.layer_norm.weight', 'extractor.block.9.layer.0.layer_norm.weight', 'extractor.block.8.layer.1.DenseReluDense.wo.weight', 'extractor.block.6.layer.0.SelfAttention.k.weight', 'extractor.block.10.layer.0.SelfAttention.v.weight', 'extractor.block.1.layer.0.SelfAttention.q.weight', 'extractor.block.3.layer.0.SelfAttention.q.weight', 'extractor.block.9.layer.0.SelfAttention.o.weight', 'extractor.block.11.layer.1.layer_norm.weight', 'extractor.block.0.layer.0.SelfAttention.k.weight', 'extractor.block.11.layer.0.SelfAttention.o.weight', 'extractor.block.6.layer.0.layer_norm.weight', 'extractor.block.6.layer.0.SelfAttention.v.weight', 'extractor.block.10.layer.1.layer_norm.weight', 'extractor.block.9.layer.0.SelfAttention.v.weight', 'extractor.block.9.layer.1.DenseReluDense.wi.weight', 'extractor.block.7.layer.0.SelfAttention.v.weight', 'extractor.block.7.layer.0.layer_norm.weight', 'extractor.block.11.layer.1.DenseReluDense.wi.weight', 'extractor.block.0.layer.0.SelfAttention.v.weight', 'extractor.block.7.layer.1.layer_norm.weight', 'extractor.block.6.layer.1.DenseReluDense.wo.weight', 'extractor.block.3.layer.1.DenseReluDense.wi.weight', 'extractor.block.5.layer.0.SelfAttention.k.weight', 'extractor.block.4.layer.0.SelfAttention.v.weight', 'extractor.block.11.layer.0.SelfAttention.k.weight', 'extractor.block.10.layer.1.DenseReluDense.wo.weight', 'extractor.block.10.layer.0.SelfAttention.k.weight', 'extractor.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'extractor.block.11.layer.0.SelfAttention.v.weight', 'extractor.block.2.layer.0.SelfAttention.o.weight', 'extractor.block.0.layer.1.DenseReluDense.wi.weight', 'extractor.block.7.layer.0.SelfAttention.o.weight', 'extractor.block.11.layer.0.SelfAttention.q.weight', 'extractor.block.10.layer.1.DenseReluDense.wi.weight', 'extractor.block.3.layer.0.layer_norm.weight', 'extractor.final_layer_norm.weight', 'extractor.block.4.layer.1.DenseReluDense.wo.weight', 'extractor.block.8.layer.0.SelfAttention.o.weight', 'extractor.block.2.layer.1.layer_norm.weight', 'extractor.block.6.layer.1.layer_norm.weight', 'extractor.block.5.layer.0.SelfAttention.v.weight', 'extractor.block.4.layer.1.DenseReluDense.wi.weight', 'extractor.block.9.layer.1.layer_norm.weight', 'extractor.block.4.layer.0.SelfAttention.q.weight', 'extractor.block.5.layer.1.DenseReluDense.wo.weight', 'extractor.block.5.layer.1.DenseReluDense.wi.weight', 'extractor.block.4.layer.0.SelfAttention.k.weight', 'extractor.block.8.layer.1.DenseReluDense.wi.weight', 'extractor.block.0.layer.1.layer_norm.weight', 'extractor.block.9.layer.0.SelfAttention.k.weight', 'extractor.block.2.layer.0.SelfAttention.v.weight', 'extractor.block.0.layer.0.SelfAttention.q.weight', 'extractor.block.2.layer.1.DenseReluDense.wo.weight', 'extractor.block.10.layer.0.SelfAttention.q.weight', 'extractor.block.9.layer.0.SelfAttention.q.weight', 'extractor.block.8.layer.0.layer_norm.weight', 'extractor.block.1.layer.0.layer_norm.weight', 'extractor.block.10.layer.0.layer_norm.weight', 'extractor.embed_tokens.weight', 'extractor.block.7.layer.1.DenseReluDense.wi.weight', 'extractor.block.3.layer.0.SelfAttention.k.weight', 'extractor.block.3.layer.0.SelfAttention.v.weight', 'extractor.block.1.layer.1.DenseReluDense.wi.weight', 'extractor.block.7.layer.1.DenseReluDense.wo.weight', 'extractor.block.1.layer.0.SelfAttention.v.weight', 'extractor.block.0.layer.1.DenseReluDense.wo.weight', 'extractor.block.5.layer.0.SelfAttention.o.weight', 'extractor.block.9.layer.1.DenseReluDense.wo.weight', 'extractor.block.11.layer.1.DenseReluDense.wo.weight', 'extractor.block.5.layer.0.SelfAttention.q.weight', 'extractor.block.2.layer.0.SelfAttention.k.weight', 'extractor.block.6.layer.0.SelfAttention.q.weight', 'extractor.block.10.layer.0.SelfAttention.o.weight', 'extractor.block.8.layer.0.SelfAttention.k.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = T5ForConditionalGenerationWithExtractor.from_pretrained(\"t5-base\")\n",
        "# model = T5ForConditionalGenerationWithExtractor.from_pretrained(\"check\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcyEfMoMvqxn"
      },
      "outputs": [],
      "source": [
        "model.extractor = copy.deepcopy(model.encoder)\n",
        "model.extractor.is_extractor = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE-vg2r7LhZv"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EqtVMAm0wqz"
      },
      "outputs": [],
      "source": [
        "def peek_weights():\n",
        "  for i, k in model.named_parameters():\n",
        "    if \"block.2.layer.0.SelfAttention.k.weight\" in i:\n",
        "      print(i)\n",
        "      print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNTjgurYKG2w"
      },
      "outputs": [],
      "source": [
        "def tokenize(input):\n",
        "  return tokenizer(input, max_length=sent_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\").input_ids.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "NVRdRzb62jzq"
      },
      "outputs": [],
      "source": [
        "def peek_output(input, context):\n",
        "  print(\"input:\", input)\n",
        "  print(\"context:\", context)\n",
        "  input_ids = tokenize(input)\n",
        "  context_ids = tokenize(context)\n",
        "  extractor_output = model.net.get_extractor_output(use_cache_context_ids=context_ids)\n",
        "  # print(extractor_output)\n",
        "  outputs = model.net.generate(input_ids=input_ids, use_cache_extractor_outputs=extractor_output, no_repeat_ngram_size=2)\n",
        "  print(outputs)\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "N5NlTZFmJ99D"
      },
      "outputs": [],
      "source": [
        "def peek_transfer_output(input, target_examplars, origin_examplars):\n",
        "  targets = ()\n",
        "  for sent in target_examplars:\n",
        "    targets += (tokenize(sent),)\n",
        "  origins = ()\n",
        "  for sent in origin_examplars:\n",
        "    origins += (tokenize(sent),)\n",
        "  input_ids = tokenize(input)\n",
        "  extractor_output = model.net.get_extractor_output(input_ids=input_ids, use_cache_origin_examplars_ids=origins, use_cache_target_examplars_ids=targets)\n",
        "  outputs = model.net.generate(input_ids=input_ids, use_cache_extractor_outputs=extractor_output, no_repeat_ngram_size=2)\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwgrVjx2HjYz",
        "outputId": "0224f667-87e1-4d7a-f750-b8f3be3d6e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.choice([False, True])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNW5wkBMGEE-"
      },
      "source": [
        "# Module and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pt2uXA9LoZn"
      },
      "outputs": [],
      "source": [
        "def drop_noise(sent, drop_rate=0.2):\n",
        "  for i in range(int(((sent > 1).sum() * drop_rate))):\n",
        "    randIdx = np.random.choice(np.where((sent > 1).cpu())[0])\n",
        "    sent = torch.concat((sent[:randIdx], sent[randIdx + 1:]))\n",
        "  return sent\n",
        "\n",
        "\n",
        "special_tokens_set = set(tokenizer.all_special_ids)\n",
        "\n",
        "def rand_token():\n",
        "  t = np.random.randint(tokenizer.vocab_size)\n",
        "  if t in special_tokens_set:\n",
        "    return rand_token()\n",
        "  return t\n",
        "\n",
        "\n",
        "def add_noise(sent, drop_rate=0.4):\n",
        "  for i in range(int(((sent > 1).sum() * drop_rate))):\n",
        "    randIdx = np.random.choice(np.where((sent > 1).cpu())[0])\n",
        "    sent = torch.concat((sent[:randIdx], torch.tensor([rand_token()]).cuda(), sent[randIdx:]))\n",
        "  return sent\n",
        "\n",
        "def pad_sent(sent, target=sent_length):\n",
        "  if sent.shape[0] > target:\n",
        "    return sent[:target]\n",
        "  return torch.concat((sent, torch.zeros(target - sent.shape[0], dtype=torch.long).cuda()))\n",
        "\n",
        "# def drop_noise_(sent, drop_rate=0.4):\n",
        "#   for i in range(int(sent.shape[0] * drop_rate)):\n",
        "#     randIdx = np.random.randint(sent.shape[0])\n",
        "#     sent = torch.concat((sent[:randIdx], sent[randIdx + 1:]))\n",
        "#   return sent\n",
        "\n",
        "def apply_noise(sents):\n",
        "  res = ()\n",
        "  for i, sent in enumerate(sents):\n",
        "    sent = drop_noise(sent)\n",
        "    sent = add_noise(sent)\n",
        "    sent = pad_sent(sent)\n",
        "    res += (sent,)\n",
        "  return torch.vstack(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF8g_Sc_wTBK"
      },
      "outputs": [],
      "source": [
        "class TextSettrModel(LightningModule):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.net = T5ForConditionalGenerationWithExtractor.from_pretrained(\"t5-base\")\n",
        "      self.net.extractor = copy.deepcopy(self.net.encoder)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "      context_ids, input_ids = batch[0], batch[1]\n",
        "      noisy_input_ids = apply_noise(input_ids)\n",
        "      if np.random.choice([False, True]):\n",
        "        # Noisy back translation\n",
        "        noisy_input_ids = self.net.generate(input_ids=noisy_input_ids, use_cache_extractor_outputs=0, do_sample=True, max_length=sent_length, min_length=sent_length)\n",
        "      extractor_output = self.net.get_extractor_output(use_cache_context_ids=context_ids)\n",
        "      return self.net(input_ids=noisy_input_ids, labels = input_ids, use_cache_extractor_outputs=extractor_output).loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "      context_ids, input_ids = batch[0], batch[1]\n",
        "      noisy_input_ids = apply_noise(input_ids)\n",
        "      noisy_input_ids = self.net.generate(input_ids=noisy_input_ids, use_cache_extractor_outputs=0, do_sample=True, max_length=sent_length, min_length=sent_length)\n",
        "      extractor_output = self.net.get_extractor_output(use_cache_context_ids=context_ids)\n",
        "      self.log(\"val_loss\", self.net(input_ids=noisy_input_ids, labels = input_ids, use_cache_extractor_outputs=extractor_output).loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.net.parameters(), 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEPVPN5SfPZr"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350,
          "referenced_widgets": [
            "d1798c75f758484eaba4d7ee73787160",
            "6bf95a735d084e549a605f4df1eda7c2",
            "677af1a05cc547bdbac158cd122d256b",
            "9a44dcedade04e8695642b6b1524228b",
            "44365354878641be8a13fc988c6341e2",
            "9354ed6985ff4161ba3b77eeadade741"
          ]
        },
        "id": "4s0068ofFgGA",
        "outputId": "901c50ff-f94a-47a0-af7a-99c227ff7060"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of T5ForConditionalGenerationWithExtractor were not initialized from the model checkpoint at t5-base and are newly initialized: ['extractor.block.4.layer.0.SelfAttention.o.weight', 'extractor.block.5.layer.0.layer_norm.weight', 'extractor.block.8.layer.0.SelfAttention.q.weight', 'extractor.block.1.layer.1.layer_norm.weight', 'extractor.block.1.layer.1.DenseReluDense.wo.weight', 'extractor.block.11.layer.0.layer_norm.weight', 'extractor.block.8.layer.1.layer_norm.weight', 'extractor.block.6.layer.0.SelfAttention.o.weight', 'extractor.block.0.layer.0.layer_norm.weight', 'extractor.block.0.layer.0.SelfAttention.o.weight', 'extractor.block.6.layer.1.DenseReluDense.wi.weight', 'extractor.block.3.layer.1.DenseReluDense.wo.weight', 'extractor.block.4.layer.0.layer_norm.weight', 'extractor.block.1.layer.0.SelfAttention.k.weight', 'extractor.block.3.layer.0.SelfAttention.o.weight', 'extractor.block.7.layer.0.SelfAttention.k.weight', 'extractor.block.7.layer.0.SelfAttention.q.weight', 'extractor.block.3.layer.1.layer_norm.weight', 'extractor.block.8.layer.0.SelfAttention.v.weight', 'extractor.block.2.layer.1.DenseReluDense.wi.weight', 'extractor.block.1.layer.0.SelfAttention.o.weight', 'extractor.block.4.layer.1.layer_norm.weight', 'extractor.block.2.layer.0.SelfAttention.q.weight', 'extractor.block.5.layer.1.layer_norm.weight', 'extractor.block.2.layer.0.layer_norm.weight', 'extractor.block.9.layer.0.layer_norm.weight', 'extractor.block.8.layer.1.DenseReluDense.wo.weight', 'extractor.block.6.layer.0.SelfAttention.k.weight', 'extractor.block.10.layer.0.SelfAttention.v.weight', 'extractor.block.1.layer.0.SelfAttention.q.weight', 'extractor.block.3.layer.0.SelfAttention.q.weight', 'extractor.block.9.layer.0.SelfAttention.o.weight', 'extractor.block.11.layer.1.layer_norm.weight', 'extractor.block.0.layer.0.SelfAttention.k.weight', 'extractor.block.11.layer.0.SelfAttention.o.weight', 'extractor.block.6.layer.0.layer_norm.weight', 'extractor.block.6.layer.0.SelfAttention.v.weight', 'extractor.block.10.layer.1.layer_norm.weight', 'extractor.block.9.layer.0.SelfAttention.v.weight', 'extractor.block.9.layer.1.DenseReluDense.wi.weight', 'extractor.block.7.layer.0.SelfAttention.v.weight', 'extractor.block.7.layer.0.layer_norm.weight', 'extractor.block.11.layer.1.DenseReluDense.wi.weight', 'extractor.block.0.layer.0.SelfAttention.v.weight', 'extractor.block.7.layer.1.layer_norm.weight', 'extractor.block.6.layer.1.DenseReluDense.wo.weight', 'extractor.block.3.layer.1.DenseReluDense.wi.weight', 'extractor.block.5.layer.0.SelfAttention.k.weight', 'extractor.block.4.layer.0.SelfAttention.v.weight', 'extractor.block.11.layer.0.SelfAttention.k.weight', 'extractor.block.10.layer.1.DenseReluDense.wo.weight', 'extractor.block.10.layer.0.SelfAttention.k.weight', 'extractor.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'extractor.block.11.layer.0.SelfAttention.v.weight', 'extractor.block.2.layer.0.SelfAttention.o.weight', 'extractor.block.0.layer.1.DenseReluDense.wi.weight', 'extractor.block.7.layer.0.SelfAttention.o.weight', 'extractor.block.11.layer.0.SelfAttention.q.weight', 'extractor.block.10.layer.1.DenseReluDense.wi.weight', 'extractor.block.3.layer.0.layer_norm.weight', 'extractor.final_layer_norm.weight', 'extractor.block.4.layer.1.DenseReluDense.wo.weight', 'extractor.block.8.layer.0.SelfAttention.o.weight', 'extractor.block.2.layer.1.layer_norm.weight', 'extractor.block.6.layer.1.layer_norm.weight', 'extractor.block.5.layer.0.SelfAttention.v.weight', 'extractor.block.4.layer.1.DenseReluDense.wi.weight', 'extractor.block.9.layer.1.layer_norm.weight', 'extractor.block.4.layer.0.SelfAttention.q.weight', 'extractor.block.5.layer.1.DenseReluDense.wo.weight', 'extractor.block.5.layer.1.DenseReluDense.wi.weight', 'extractor.block.4.layer.0.SelfAttention.k.weight', 'extractor.block.8.layer.1.DenseReluDense.wi.weight', 'extractor.block.0.layer.1.layer_norm.weight', 'extractor.block.9.layer.0.SelfAttention.k.weight', 'extractor.block.2.layer.0.SelfAttention.v.weight', 'extractor.block.0.layer.0.SelfAttention.q.weight', 'extractor.block.2.layer.1.DenseReluDense.wo.weight', 'extractor.block.10.layer.0.SelfAttention.q.weight', 'extractor.block.9.layer.0.SelfAttention.q.weight', 'extractor.block.8.layer.0.layer_norm.weight', 'extractor.block.1.layer.0.layer_norm.weight', 'extractor.block.10.layer.0.layer_norm.weight', 'extractor.embed_tokens.weight', 'extractor.block.7.layer.1.DenseReluDense.wi.weight', 'extractor.block.3.layer.0.SelfAttention.k.weight', 'extractor.block.3.layer.0.SelfAttention.v.weight', 'extractor.block.1.layer.1.DenseReluDense.wi.weight', 'extractor.block.7.layer.1.DenseReluDense.wo.weight', 'extractor.block.1.layer.0.SelfAttention.v.weight', 'extractor.block.0.layer.1.DenseReluDense.wo.weight', 'extractor.block.5.layer.0.SelfAttention.o.weight', 'extractor.block.9.layer.1.DenseReluDense.wo.weight', 'extractor.block.11.layer.1.DenseReluDense.wo.weight', 'extractor.block.5.layer.0.SelfAttention.q.weight', 'extractor.block.2.layer.0.SelfAttention.k.weight', 'extractor.block.6.layer.0.SelfAttention.q.weight', 'extractor.block.10.layer.0.SelfAttention.o.weight', 'extractor.block.8.layer.0.SelfAttention.k.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name | Type                                    | Params\n",
            "-----------------------------------------------------------------\n",
            "0 | net  | T5ForConditionalGenerationWithExtractor | 332 M \n",
            "-----------------------------------------------------------------\n",
            "332 M     Trainable params\n",
            "0         Non-trainable params\n",
            "332 M     Total params\n",
            "1,330.128 Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1798c75f758484eaba4d7ee73787160",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation sanity check: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9354ed6985ff4161ba3b77eeadade741",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "model = TextSettrModel()\n",
        "module = AmazonDataModule()\n",
        "# checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=root, filename='{epoch}')\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\")\n",
        "logger = TensorBoardLogger(\"logs\", name=\"style_transfer\")\n",
        "trainer = Trainer(max_epochs = 10, gpus=1, default_root_dir=root, val_check_interval=0.25, precision=32, logger=logger, resume_from_checkpoint = f\"{root}/10-hour.ckpt)\n",
        "trainer.fit(model,module)\n",
        "# model.net.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6k31qPwHMLUX"
      },
      "outputs": [],
      "source": [
        "# model.net\n",
        "trainer.save_checkpoint(f\"{root}/10-hour.ckpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI6KQ9vpfYsM"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/style_transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "m-jiJmxQ4xce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "JewFvfsmG9Qh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "cf2451a7-a72b-4373-bee3-a432e1a7f247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: This was a thought-provoking read\n",
            "context: \n",
            "tensor([[    0,   100,    47,     3,     9,   816,    18, 28268,   608,     5,\n",
            "             1]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This was a thought-provoking read.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 183
        }
      ],
      "source": [
        "peek_output(\"This was a thought-provoking read\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "dDxw6gNBHaRq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "95bd8557-7a7d-4fdd-d836-968b0bf0991b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i hereby gonna never purchase anything from this seller in the future'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 182
        }
      ],
      "source": [
        "formal_examplars = [\"This was a remarkably thought-provoking read.\",\n",
        "                  \"It is certainly amongst my favorites.\"\n",
        "                  \"We humbly request your presence at our gala on the 12th.\"]\n",
        "informal_examplars = [\"reading this rly makes u think\",\n",
        "                      \"Its def one of my favs\",\n",
        "                      \"come swing by our bbq next week if ya can make it\"]\n",
        "formal_input = \"I hereby commit to never purchase anything from this institution in the future.\"\n",
        "lambda_factor = 5\n",
        "peek_transfer_output(formal_input, informal_examplars, formal_examplars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "FDCZW0sVd6Is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b683bf7-9ab1-49dc-adc0-e260d7893dd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"I I couldn't figure out what the author was trying to say.\",\n",
              " 'I couldnt figure out what the author was trying to say.')"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ],
      "source": [
        "formal_examplars = [\"This was a remarkably thought-provoking read.\",\n",
        "                  \"It is certainly amongst my favorites.\"\n",
        "                  \"We humbly request your presence at our gala on the 12th.\"]\n",
        "informal_examplars = [\"reading this rly makes u think\",\n",
        "                      \"Its def one of my favs\",\n",
        "                      \"come swing by our bbq next week if ya can make it\"]\n",
        "formal_input = \"I couldn’t figure out what the author was trying to say.\"\n",
        "lambda_factor = 3\n",
        "peek_transfer_output(formal_input, formal_examplars, informal_examplars),peek_transfer_output(formal_input, informal_examplars, formal_examplars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "AoNy2olqLmNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b39f44a-35a4-4076-e3a1-e94d60ab49d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Please get it!', 'Please get it. Please do get this.')"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ],
      "source": [
        "orig_ex = [          \n",
        "\"No thank you, I'd prefer not to.\",\n",
        "\"This game could have been better designed.\",\n",
        "\"Do you know why they might have delayed the launch?\",\n",
        "\"Sorry, I wasn' certain if you were joking.\"\n",
        "]\n",
        "\n",
        "targ_ex = [\n",
        "\"Hell no, you can't make me do that.\",\n",
        "\"This game is such a piece of garbage!\",\n",
        "\"Why in god's name would they delay the damn launch? Are you frigging kidding me?\"\n",
        "]\n",
        "\n",
        "sent_ex = \"Please get \"\n",
        "lambda_factor = 3\n",
        "peek_transfer_output(sent_ex, targ_ex, orig_ex), peek_transfer_output(sent_ex, orig_ex, targ_ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "5srJUDxCrHwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fab24e1-106b-448b-897e-e903877b8188"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0       windows have n't been cleaned in years you can see scum on them .\n",
              " 1                                                   waitresses are slow .\n",
              " 2                                        just a mess avoid at all costs !\n",
              " 3                                                                   bad !\n",
              " 4     now pizza is beyond awful and wings are down there with its level .\n",
              "                                      ...                                 \n",
              " 95    no touching , no going ahead of people , no laughter , no dancing .\n",
              " 96                                                                _num_ .\n",
              " 97                                                                _num_ .\n",
              " 98                                                                _num_ .\n",
              " 99                                                                _num_ .\n",
              " Name: 0, Length: 100, dtype: object,\n",
              " 0                         these donuts have the perfect texture and taste .\n",
              " 1                                                 good food for the price .\n",
              " 2     a little dirty on the inside , but wonderful people that work there !\n",
              " 3              i always order it when i go there and it is always awesome .\n",
              " 4          the rest of the food there is good also and not very expensive .\n",
              "                                       ...                                  \n",
              " 95                           it has better food than these larger buffets .\n",
              " 96                                this is the best barber shop in phoenix .\n",
              " 97                      got a good haircut , everything was nice and even .\n",
              " 98                                    friendly staff , and got in and out .\n",
              " 99                                                 i just love this place !\n",
              " Name: 0, Length: 100, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ],
      "source": [
        "n_ex = 100\n",
        "neg_ex = pd.read_csv(f\"{root}/yelp/neg.csv\", sep=\"\\t\").get(\"0\")[:n_ex]\n",
        "pos_ex  =  pd.read_csv(f\"{root}/yelp/pos.csv\", sep=\"\\t\").get(\"0\")[:n_ex]\n",
        "neg_ex, pos_ex"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex_sent = \"The product is \"\n",
        "lambda_factor = 3\n",
        "peek_transfer_output(ex_sent, neg_ex, pos_ex), peek_transfer_output(ex_sent, pos_ex, neg_ex)"
      ],
      "metadata": {
        "id": "ElsGm6SAyUFV",
        "outputId": "8131970c-2844-4f66-a55b-d48604d81c42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('The product is defective.', 'The product is good The Product is excellent')"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex_sent = \"Apple watch is\"\n",
        "peek_transfer_output(ex_sent, neg_ex, pos_ex), peek_transfer_output(ex_sent, pos_ex, neg_ex)"
      ],
      "metadata": {
        "id": "-8UWKSgQzPjd",
        "outputId": "fa0bd2e0-fe59-40e8-8859-9c8fd1bd459e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Apple watch is useless.', 'Apple watch is amazing.')"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex_sent = \"The University of California at San Diego is\"\n",
        "lambda_factor = 5\n",
        "peek_transfer_output(ex_sent, neg_ex, pos_ex), peek_transfer_output(ex_sent, pos_ex, neg_ex)"
      ],
      "metadata": {
        "id": "H0n9fg6R0FpC",
        "outputId": "c9954f05-13e4-46d1-8e6f-200d28dcb572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('The University of California at San Diego is a dead zone.',\n",
              " 'The University of California at San Diego is beautiful.')"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "american_ex = [\n",
        "               \"It cost ten bucks.\",\n",
        "              \"My neighbor apologized.\",\n",
        "            \"I'm heading out to the bar with some friends.\"\n",
        "]\n",
        "british_ex = [\n",
        "  \"It cost ten quid.\",\n",
        "  \"My neighbour apologised.\",\n",
        "  \"I'm heading out LO the pub with some mates.\"\n",
        "]\n",
        "sent_ex = \"My favourite food: \"\n",
        "lambda_factor = 10\n",
        "peek_transfer_output(sent_ex, american_ex, british_ex)"
      ],
      "metadata": {
        "id": "vm4sgMtX0Ybl",
        "outputId": "f35a18f2-8e81-41ad-aa76-b8585a2afaf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My favourite food: My favorite food was Wendy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "text_settr.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d1798c75f758484eaba4d7ee73787160": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bf95a735d084e549a605f4df1eda7c2",
              "IPY_MODEL_677af1a05cc547bdbac158cd122d256b",
              "IPY_MODEL_9a44dcedade04e8695642b6b1524228b"
            ],
            "layout": "IPY_MODEL_44365354878641be8a13fc988c6341e2"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}