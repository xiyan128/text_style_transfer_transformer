
@article{dai_style_2019,
  title        = {Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation},
  url          = {http://arxiv.org/abs/1905.05621},
  shorttitle   = {Style Transformer},
  abstract     = {Disentangling the content and style in the latent space is prevalent in unpaired text style transfer. However, two major issues exist in most of the current neural models. 1) It is difﬁcult to completely strip the style information from the semantics for a sentence. 2) The recurrent neural network ({RNN}) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation. Source code will be available on Github1.},
  journaltitle = {{arXiv}:1905.05621 [cs]},
  author       = {Dai, Ning and Liang, Jianze and Qiu, Xipeng and Huang, Xuanjing},
  urldate      = {2022-03-15},
  date         = {2019-08-20},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1905.05621},
  keywords     = {Computer Science - Computation and Language},
  file         = {Dai et al. - 2019 - Style Transformer Unpaired Text Style Transfer wi.pdf:/Users/xiyan/Zotero/storage/P8SE4JB2/Dai et al. - 2019 - Style Transformer Unpaired Text Style Transfer wi.pdf:application/pdf}
}
riley_textsettr_2021
@article{raffel_exploring_2020,
  title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  url          = {http://arxiv.org/abs/1910.10683},
  abstract     = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing ({NLP}). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for {NLP} by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for {NLP}, we release our data set, pre-trained models, and code.},
  journaltitle = {{arXiv}:1910.10683 [cs, stat]},
  author       = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  urldate      = {2022-03-16},
  date         = {2020-07-28},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1910.10683},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
  file         = {Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf:/Users/xiyan/Zotero/storage/AAUG8U7N/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf}
}

@article{riley_textsettr_2021,
  title        = {{TextSETTR}: Few-Shot Text Style Extraction and Tunable Targeted Restyling},
  url          = {http://arxiv.org/abs/2010.03802},
  shorttitle   = {{TextSETTR}},
  abstract     = {We present a novel approach to the problem of text style transfer. Unlike previous approaches requiring style-labeled training data, our method makes use of readily-available unlabeled text by relying on the implicit connection in style between adjacent sentences, and uses labeled data only at inference time. We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to extract a style vector from text and use it to condition the decoder to perform style transfer. As our labelfree training results in a style vector space encoding many facets of style, we recast transfers as “targeted restyling” vector operations that adjust speciﬁc attributes of the input while preserving others. We demonstrate that training on unlabeled Amazon reviews data results in a model that is competitive on sentiment transfer, even compared to models trained fully on labeled data. Furthermore, applying our novel method to a diverse corpus of unlabeled web text results in a single model capable of transferring along multiple dimensions of style (dialect, emotiveness, formality, politeness, sentiment) despite no additional training and using only a handful of exemplars at inference time.},
  journaltitle = {{arXiv}:2010.03802 [cs]},
  author       = {Riley, Parker and Constant, Noah and Guo, Mandy and Kumar, Girish and Uthus, David and Parekh, Zarana},
  urldate      = {2022-03-04},
  date         = {2021-06-23},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {2010.03802},
  keywords     = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  file         = {Riley et al. - 2021 - TextSETTR Few-Shot Text Style Extraction and Tuna.pdf:/Users/xiyan/Zotero/storage/LWRKCFY5/Riley et al. - 2021 - TextSETTR Few-Shot Text Style Extraction and Tuna.pdf:application/pdf}
}